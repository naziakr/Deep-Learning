# Take this dataset for bank customer churn prediction : https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling

df = pd.read_csv("/kaggle/input/bank-customer-churn-modeling/Churn_Modelling.csv")

df.head()
RowNumber	CustomerId	Surname	CreditScore	Geography	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Exited
0	1	15634602	Hargrave	619	France	Female	42	2	0.00	1	1	1	101348.88	1
1	2	15647311	Hill	608	Spain	Female	41	1	83807.86	1	0	1	112542.58	0
2	3	15619304	Onio	502	France	Female	42	8	159660.80	3	1	0	113931.57	1
3	4	15701354	Boni	699	France	Female	39	1	0.00	2	0	0	93826.63	0
4	5	15737888	Mitchell	850	Spain	Female	43	2	125510.82	1	1	1	79084.10	0
What we have to do: 1) Build a deep learning model to predict churn rate at bank. 2) Once model is built, print classification report and analyze precision, recall and f1-score


Start with Data Cleaning:
Drop rows that server no purpose - RowNumber, CustimerId, Surname


df.drop('RowNumber',axis='columns',inplace=True)
df.drop('CustomerId',axis='columns',inplace=True)
df.drop('Surname',axis='columns',inplace=True)

df.head()
CreditScore	Geography	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Exited
0	619	France	Female	42	2	0.00	1	1	1	101348.88	1
1	608	Spain	Female	41	1	83807.86	1	0	1	112542.58	0
2	502	France	Female	42	8	159660.80	3	1	0	113931.57	1
3	699	France	Female	39	1	0.00	2	0	0	93826.63	0
4	850	Spain	Female	43	2	125510.82	1	1	1	79084.10	0


Data Visualisation

Churn based on Tenure


tenure_churn_no = df[df.Exited==0].Tenure
tenure_churn_yes = df[df.Exited==1].Tenure

plt.xlabel("Tenure")
plt.ylabel("Number Of Customers")
plt.title("Customer Churn Prediction Visualiztion")


plt.hist([tenure_churn_yes, tenure_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])
plt.legend()
tenure_churn_no = df[df.Exited==0].Tenure
tenure_churn_yes = df[df.Exited==1].Tenure
​
plt.xlabel("Tenure")
plt.ylabel("Number Of Customers")
plt.title("Customer Churn Prediction Visualiztion")
​
​
plt.hist([tenure_churn_yes, tenure_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])
plt.legend();


tenure_churn_no = df[df.Exited==0].EstimatedSalary
tenure_churn_yes = df[df.Exited==1].EstimatedSalary

plt.xlabel("EstimatedSalary")
plt.ylabel("Number Of Customers")
plt.title("Customer Churn Prediction Visualiztion")


plt.hist([tenure_churn_yes, tenure_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])
plt.legend();
tenure_churn_no = df[df.Exited==0].EstimatedSalary
tenure_churn_yes = df[df.Exited==1].EstimatedSalary
​
plt.xlabel("EstimatedSalary")
plt.ylabel("Number Of Customers")
plt.title("Customer Churn Prediction Visualiztion")
​
​
plt.hist([tenure_churn_yes, tenure_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])
plt.legend();


There is very little significance of EstimatedSalary on Churn


Balance
tenure_churn_no = df[df.Exited==0].Balance
tenure_churn_yes = df[df.Exited==1].Balance
​
plt.xlabel("Balance")
plt.ylabel("Number Of Customers")
plt.title("Customer Churn Prediction Visualiztion")
​
​
plt.hist([tenure_churn_yes, tenure_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])
plt.legend();


People with balance betwer ~70000 and 200000 have low EXit rate


tenure_churn_no = df[df.Exited==0].Geography
tenure_churn_yes = df[df.Exited==1].Geography
plt.xlabel("Geography")
plt.ylabel("Number Of Customers")
plt.title("Customer Churn Prediction Visualiztion")
​
​
plt.hist([tenure_churn_yes, tenure_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])
plt.legend();


Check the data types we are dealing with


df.dtypes
CreditScore          int64
Geography           object
Gender              object
Age                  int64
Tenure               int64
Balance            float64
NumOfProducts        int64
HasCrCard            int64
IsActiveMember       int64
EstimatedSalary    float64
Exited               int64
dtype: object

We see Geography and Gender is categorical. These need to be sorted

add Codeadd Markdown
print_unique_col_values(df)
def print_unique_col_values(df):
       for column in df:
            if df[column].dtypes=='object':
                print(f'{column}: {df[column].unique()}') 

print_unique_col_values(df)
Geography: ['France' 'Spain' 'Germany']
Gender: ['Female' 'Male']

There are two ways to sort this. We can do mapping or simply use on-hot-encoding


df['Gender'].replace({'Female':1,'Male':0},inplace=True)
add Codeadd Markdown
df['Geography'].replace({'France':0, 'Spain':1, 'Germany':2},inplace=True)

What does the DataFrame now looks like?


df.head()
CreditScore	Geography	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Exited
0	619	0	1	42	2	0.00	1	1	1	101348.88	1
1	608	1	1	41	1	83807.86	1	0	1	112542.58	0
2	502	0	1	42	8	159660.80	3	1	0	113931.57	1
3	699	0	1	39	1	0.00	2	0	0	93826.63	0
4	850	1	1	43	2	125510.82	1	1	1	79084.10	0

Checkng datatypes now


df.dtypes
df.dtypes
CreditScore          int64
Geography            int64
Gender               int64
Age                  int64
Tenure               int64
Balance            float64
NumOfProducts        int64
HasCrCard            int64
IsActiveMember       int64
EstimatedSalary    float64
Exited               int64
dtype: object

Train-test-split

cols_to_scale = ['Tenure','Balance','EstimatedSalary']
​
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])

X = df.drop('Exited',axis='columns')
y = df['Exited']
​
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=5)

Build a model in TensorFlow


X_train.shape
(8000, 10)
add Codeadd Markdown
import tensorflow as tf
from tensorflow import keras
​
​
model = keras.Sequential([
    keras.layers.Dense(10, input_shape=(10,), activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])
​
opt = keras.optimizers.Adam(learning_rate=0.01)
​
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
​
model.fit(X_train, y_train, epochs=50)
Epoch 1/50
250/250 [==============================] - 1s 2ms/step - loss: 2.0732 - accuracy: 0.6501
Epoch 2/50
250/250 [==============================] - 1s 2ms/step - loss: 0.6274 - accuracy: 0.7632
Epoch 3/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4934 - accuracy: 0.7928
Epoch 4/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4822 - accuracy: 0.7905
Epoch 5/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4827 - accuracy: 0.7914
Epoch 6/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4757 - accuracy: 0.7993
Epoch 7/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4697 - accuracy: 0.7995
Epoch 8/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4639 - accuracy: 0.8015
Epoch 9/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4645 - accuracy: 0.7983
Epoch 10/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4739 - accuracy: 0.7962
Epoch 11/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4792 - accuracy: 0.7946
Epoch 12/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4656 - accuracy: 0.7997
Epoch 13/50
250/250 [==============================] - 1s 3ms/step - loss: 0.4601 - accuracy: 0.8058
Epoch 14/50
250/250 [==============================] - 1s 3ms/step - loss: 0.4636 - accuracy: 0.7987
Epoch 15/50
250/250 [==============================] - 1s 3ms/step - loss: 0.4510 - accuracy: 0.8043
Epoch 16/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4583 - accuracy: 0.8004
Epoch 17/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4616 - accuracy: 0.8026
Epoch 18/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4569 - accuracy: 0.8048
Epoch 19/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4584 - accuracy: 0.8009
Epoch 20/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4683 - accuracy: 0.7990
Epoch 21/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4612 - accuracy: 0.8033
Epoch 22/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4680 - accuracy: 0.8011
Epoch 23/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4625 - accuracy: 0.8026
Epoch 24/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4616 - accuracy: 0.8036
Epoch 25/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4697 - accuracy: 0.8016
Epoch 26/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4545 - accuracy: 0.8048
Epoch 27/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4619 - accuracy: 0.8005
Epoch 28/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4775 - accuracy: 0.8009
Epoch 29/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4632 - accuracy: 0.8027
Epoch 30/50
250/250 [==============================] - 1s 3ms/step - loss: 0.4516 - accuracy: 0.8083
Epoch 31/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4698 - accuracy: 0.7990
Epoch 32/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4689 - accuracy: 0.8012
Epoch 33/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4579 - accuracy: 0.8044
Epoch 34/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4553 - accuracy: 0.8036
Epoch 35/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4482 - accuracy: 0.8094
Epoch 36/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4496 - accuracy: 0.8089
Epoch 37/50
250/250 [==============================] - 1s 3ms/step - loss: 0.4507 - accuracy: 0.8069
Epoch 38/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4506 - accuracy: 0.8089
Epoch 39/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4562 - accuracy: 0.8052
Epoch 40/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4495 - accuracy: 0.8081
Epoch 41/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4446 - accuracy: 0.8133
Epoch 42/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4522 - accuracy: 0.8089
Epoch 43/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4516 - accuracy: 0.8094
Epoch 44/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4453 - accuracy: 0.8136
Epoch 45/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4460 - accuracy: 0.8102
Epoch 46/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4486 - accuracy: 0.8102
Epoch 47/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4496 - accuracy: 0.8080
Epoch 48/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4454 - accuracy: 0.8155
Epoch 49/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4494 - accuracy: 0.8099
Epoch 50/50
250/250 [==============================] - 1s 2ms/step - loss: 0.4401 - accuracy: 0.8140
<keras.callbacks.History at 0x7c63481c9270>

yp = model.predict(X_test)
yp[:5]
63/63 [==============================] - 0s 2ms/step
array([[0.06151482],
       [0.06127609],
       [0.03404324],
       [0.03525555],
       [0.09320056]], dtype=float32)

y_pred = []
for element in yp:
    if element > 0.5:
        y_pred.append(1)
    else:
        y_pred.append(0)

y_pred[:10]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

Classification Report

from sklearn.metrics import confusion_matrix , classification_report
​
print(classification_report(y_test,y_pred))
              precision    recall  f1-score   support

           0       0.81      0.99      0.89      1595
           1       0.74      0.07      0.13       405

    accuracy                           0.81      2000
   macro avg       0.77      0.53      0.51      2000
weighted avg       0.79      0.81      0.74      2000


Plot confusion matrix

import seaborn as sn
cm = tf.math.confusion_matrix(labels=y_test,predictions=y_pred)
​
plt.figure(figsize = (10,7))
sn.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')
Text(95.72222222222221, 0.5, 'Truth')

